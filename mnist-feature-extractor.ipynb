{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor on MNIST\n",
    "\n",
    "In this notebook I'd like to use a pretrained model to extract features from the MNIST dataset. I'll use the pretrained SqueezeNet model from PyTorch's torchvision package.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.1\n",
      "Torchvision Version:  0.14.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "# data_dir = \"./data/hymenoptera_data\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 4\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 3          #15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's start by loading a pretrained model (i.e. squeezenet) and modifying the last layer to classify 10 classes instead of 1000.\n",
    "\n",
    "# read the train csv file\n",
    "train_df = pd.read_csv('../datasets/digit-recognizer/train.csv')\n",
    "test_df = pd.read_csv('../datasets/digit-recognizer/test.csv')\n",
    "\n",
    "# get the labels\n",
    "labels = train_df['label'].values\n",
    "\n",
    "# drop the label column\n",
    "train_df.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the images\n",
    "images = train_df.values\n",
    "\n",
    "# reshape the images\n",
    "images = images.reshape((-1, 28, 28))\n",
    "\n",
    "# plot some images\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        ax[i, j].imshow(images[i*5 + j], cmap='gray')\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (33600, 28, 28)\n",
      "X_val.shape: (8400, 28, 28)\n",
      "y_train.shape: (33600,)\n",
      "y_val.shape: (8400,)\n"
     ]
    }
   ],
   "source": [
    "# create our train and validation splits\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print shapes of train test split\n",
    "print(f'X_train.shape: {X_train.shape}')\n",
    "print(f'X_val.shape: {X_val.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'y_val.shape: {y_val.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # add transform to convert grayscale to rgb\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        # add transform to convert grayscale to rgb\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the train model function below to not use dataloaders\n",
    "def train_model(model, x, y, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in zip(x, y):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(x)\n",
    "            epoch_acc = running_corrects.double() / len(x)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "x = X_train[:5]\n",
    "y = y_train[:5]\n",
    "\n",
    "# x = torch.tensor(x, dtype=torch.float32)\n",
    "# y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "x_0 = np.asarray(x[0], dtype=np.uint8)\n",
    "\n",
    "# x_0 = np.asarray(x_0, dtype=np.uint8)\n",
    "\n",
    "print(x_0.dtype, x_0.shape)\n",
    "\n",
    "x_0 = data_transforms['train'](x_0)\n",
    "\n",
    "\n",
    "\n",
    "# img = Image.open(input_path)\n",
    "\n",
    "# print(f'img.shape before transform: {img.size}')\n",
    "\n",
    "# # Use torch data transform to resize image\n",
    "# img = data_transforms['val'](img)\n",
    "\n",
    "# Train and evaluate\n",
    "# model_ft, hist = train_model(model_ft, x, y, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for this run\n",
    "num_classes = 10    # 10 classes for digits 0-9 of MNIST dataset\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)\n",
    "\n",
    "# put model in training mode\n",
    "model_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_ft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1f/qjk02_3s70zd4sxj573fj7400000gn/T/ipykernel_53684/1663090423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ft' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "x = X_train[:5]\n",
    "y = y_train[:5]\n",
    "\n",
    "print(num_epochs)\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, x, y, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
